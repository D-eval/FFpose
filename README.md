# Forward-Forward Poses Classification

损失函数不是通过反向传播得到的梯度来更新参数，而是通过直接在网络中进行正向传播时调整网络的激活输出。

假设某一层的好度函数是该层经过修正线性单元（rectified linear neurons）的活动值的平方和。学习的目标是使真实数据的好度高于某个阈值，而负数据的好度低于该阈值。更具体地，目标是正确分类输入向量为正数据或负数据，其中输入向量为正的概率（即真实数据）由如下逻辑函数给出：

$$ p(positive) = \sigma (\sum_j y_j^2 - \theta) $$

其中，$y_j$是隐藏单元 $j$ 在层归一化前的活动值，$\sigma$是logistic函数，$\theta$ 是阈值。负数据可以通过神经网络的自上而下连接来预测，或者可以外部提供。

归一化移除了所有用于确定第一隐藏层goodness的activity信息，迫使下一层隐藏层使用第一层神经元活动的相对信息。相对活动在进行层归一化后不会受到影响。换句话说，第一层隐藏层的活动向量有长度和方向。长度用于定义该层的好度，而只有方向信息被传递到下一层。

## MoE

MoE被广泛的应用在条件生成任务中，其拥有多个结构相同参数不同的子模块（即专家，通常是生成模型）用来处理不同的人物和一个：给定一个输入，如一句话，由一个门控单元决定该由哪个子模块去激活哪些专家：它根据输入为每个专家输出一个logit，作为该专家是否被激活的依据。门控网络为每个专家输出一个logit，通过 top-k 选取得分最高的k个专家参与计算（如 top-2），并对其输出加权求和作为最终输出。因此虽然MoE参数量大，却有稀疏激活的特点，可以减轻运算量。

MoE允许我们在小型电脑独立显卡上建立大模型。实际操作时，我们只需加载门控单元和一个专家类模块，所有专家保存为pth文件，当门控单元激活了某个专家时，我们加载这个专家的参数即可，这允许我们在小型的电脑上部署大量的专家。

关于门控单元和专家的耦合中，一个重要的问题是：门控单元对专家的决定行为是否应该随着专家的训练得到改变？还是不取决于专家，仅仅依靠门控单元自身？

在本文中，我们设置的门控单元的训练应当和专家是分开进行的。否则，如果门控单元的训练依赖了专家的反馈，会导致门控单元倾向于结果更好的专家，最后只有单独一个专家得到了充分训练，于是MoE退化为了一个普通的生成模型。后者的解决方法有很多，例如引入均匀惩罚，惩罚门控只使用少数专家。但是为了获得足够用于生成或预测任务的门控单元，我们着重讨论前者。

### 有监督情形

在一切生成式或者预测式任务中，门控单元都可以是一个finetune的分类器--对于没种输入类别决定对应的专家。

用一个finetune的分类器直接决定专家的优点是：专家的激活次数只与训练集上的类别个数有关，从而每个专家是否被训练是能人为掌控的。缺点也很明显：当类别过多时，例如服从 Weibull 分布时，我们需要太多专家，导致尾部专家不能得到充分的训练。解决方法是：提前统计每个类别的数目，设定某个阈值让门控单元拒绝这些样本。

然而这种方法过于粗暴，当我们提前知道类别的信息时，完全可以直接为每个类别配备一个专家，门控单元失去意义。

### 无监督情形

无监督学习得到的离散token通常是超越语言的抽象语义，拥有比人为标注类别更高的细粒度。模型通过用token来预测掩码来让局部表示包含更全局的信息，这种本身就用于还原数据自身结构的token是天然的。

对于姿态数据，存在较成熟的无监督学习方法如MotionBert。

## 思路

用一个人为的方式判断「走路」，作为先验信息，得到

## 讲解

每个批次加载姿态序列`BTNd`和`text`

每一个隐层只负责一个类的拉拢，而排斥其他类，作为goodness函数

先embed每个骨骼并加入`time_embedding`，得到`(B,T,N,D)`



这一层的goodness函数，选取

用spacial attention得到# FFpose
